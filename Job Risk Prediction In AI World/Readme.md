ü§ñ AI Impact on Jobs 2030: Risk Prediction using Support Vector Machine (SVM)
This project aims to predict the Risk Category of a job (Low, Medium, or High) based on various job attributes, including salary, experience, education level, and key AI exposure indices. This is formulated as a Multi-Class Classification problem, with the Support Vector Classifier (SVC) being the primary model used for prediction.The analysis confirms a strong relationship between the target variable ($\text{Risk\_Category}$) and the predictive features, resulting in a model with near-perfect accuracy on the test set.

üìÅ Project Structure-
The project is contained within a single Jupyter Notebook:Job Risk Pred (SVM).ipynb: Contains all steps from data loading, Exploratory Data Analysis (EDA), feature engineering, model training (SVC), and evaluation.

üìä Dataset and Key Insights- 
The dataset, sourced from a Kaggle competition, contains information on various job titles and their predicted risk levels concerning AI automation by 2030.Size: 3,000 rows.Quality: The data was exceptionally clean, with zero missing values and no duplicate rows, thus requiring no initial cleaning.Key Findings from EDARisk Distribution (Q2): The Medium Risk category is the largest, containing $\mathbf{1,521}$ jobs (over half the dataset). The Low ($\mathbf{739}$) and High ($\mathbf{740}$) risk categories are almost equally balanced.Risk & Automation (Q4): A direct correlation was confirmed:Low Risk jobs have a median $\text{Automation\_Probability\_2030}$ near $\mathbf{0.2}$.Medium Risk jobs have a median probability near $\mathbf{0.5}$.High Risk jobs have a median probability near $\mathbf{0.8}$.Education & Salary (Q1): A clear positive trend exists, where higher education levels generally correlate with higher average salaries (e.g., PhD salaries are highest).AI Exposure Index (Q5):Highest Exposure includes jobs like Graphic Designer, Construction Worker, and Delivery Driver.Lowest Exposure includes roles like Research Scientist, Data Analyst, and Teacher.

üõ†Ô∏è Model and MethodologyFeature Engineering and Preprocessing-
The SVC model relies on distance calculations, necessitating careful preprocessing:Target Label Encoding: The categorical target variable, $\mathbf{Y}$ ($\text{Risk\_Category}$), was converted to numerical integers ($\mathbf{0, 1, 2}$) using $\text{LabelEncoder}$.Categorical Encoding: Features like $\text{Job\_Title}$ and $\text{Education\_Level}$ were converted into a machine-readable format using $\text{OneHotEncoder}$.Numerical Scaling (Crucial for SVM): All numerical features (Salary, Experience, and Skill columns) were normalized using $\text{StandardScaler}$ to ensure fair weighting in the $\text{SVC}$'s distance-based algorithm.Model Training: Support Vector Classifier (SVC)Model: $\text{SVC}$ with a Radial Basis Function (RBF) kernel and $\mathbf{\text{class\_weight='balanced'}}$.Split: $80\%$ Training set, $20\%$ Test set (stratified).

üöÄ ResultsMetricScoreTest Set Accuracy$\mathbf{99.33\%}$Discussion on Accuracy- 
The near-perfect accuracy suggests that the $\text{Risk\_Category}$ is determined by very clear, quantifiable thresholds (e.g., on $\text{Automation\_Probability\_2030}$). The $\text{SVC}$ model, being powerful for high-dimensional classification, was able to perfectly capture these boundaries.Note: The $\mathbf{99.33\%}$ accuracy is likely overstated for real-world application. While it confirms the features are extremely strong predictors, some degradation should be expected when applying the model to messy, unlabeled, real-world data outside the provided dataset.

üí° Why SVM was Chosen-
The Support Vector Machine (SVC) was an optimal choice for this problem:High-Dimensional Space: After $\text{OneHotEncoding}$ for $\text{Job\_Title}$ and $\text{Education\_Level}$, the feature space became high-dimensional. SVMs are mathematically designed to find the optimal separation hyperplane that maximizes the margin between classes in such complex spaces.Strong Generalization: When combined with the necessary $\text{StandardScaler}$ preprocessing, SVMs provide strong generalization capabilities, making the model robust and less prone to overfitting than other algorithms might be in this scenario.
